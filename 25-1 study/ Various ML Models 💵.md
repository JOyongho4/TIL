# ğŸ’³ğŸ’² Loan Prediction w/ Various ML Models ğŸ’µ

## ê³µë¶€í•  ë‚´ìš©
- SMOTE
- Logistic Regresssion
- KNN
- SVM
- Naive Bayes
- Decision Tree
- Random Forest
- Gradient Boosting

## Smote

- í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ í•´ê²°í•˜ëŠ” ê¸°ë²•

```py
X, y = SMOTE().fit_resample(X, y)
```

X -> íƒ€ê²Ÿë³€ìˆ˜ë¥¼ ì œê±°í•œ df

y -> íƒ€ê²Ÿë³€ìˆ˜


SMOTE -> í´ë˜ìŠ¤ 50:50ìœ¼ë¡œ ë§ì¶°ì¤Œ

## Logistic Regresssion

- ë¶„ë¥˜ ë¬¸ì œë¥¼ í’€ê¸°ìœ„í•œ í†µê³„ ëª¨ë¸
- ì¶œë ¥ê°’ì€ 0ê³¼ 1ì‚¬ì´ì˜ í™•ë¥ 

```py
LRclassifier = LogisticRegression(solver='saga', max_iter=500, random_state=1)
LRclassifier.fit(X_train, y_train)

y_pred = LRclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
LRAcc = accuracy_score(y_pred,y_test)
print('LR accuracy: {:.2f}%'.format(LRAcc*100))
```

### solver = SAGA
- solverëŠ” ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ ìµœì í™” ë°©ë²•ì„ ì„ íƒí•˜ëŠ” ì˜µì…˜
- sagaë€?
    - L1,L2, Elasic Net ì •ê·œí™” ë‹¤ ê°€ëŠ¥
    - ëŒ€ê·œëª¨ ë°ì´í„°ì—ë„ ì˜ ë™ì‘
    - sparse ë°ì´í„°ë„ ì§€ì›
    - ë³´ë™ ë¹ ë¦„
    - softmaxê¹Œì§€ ì§€ì›

- L1, L2, Elastic Netë€?

    ëª¨ë¸ì„ ë‹¨ìˆœí•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ ì¶”ê°€í•˜ëŠ” ê·œì œ ë°©ë²•

    - L1, ë¼ì˜
        - ì¼ë¶€ ê°€ì¤‘ì¹˜ë¥¼ ì •í™•íˆ 0ìœ¼ë¡œ ë§Œë“¤ì–´ë²„ë¦¼
        - ê²°ê³¼ì ìœ¼ë¡œ ëª¨ë¸ì´ í¬ì†Œí•´ì§(sparse) -> ì¤‘ìš”í•˜ì§€ ì•Šì€ í”¼ì³ë¥¼ ì•„ì˜ˆ ì œê±°í•˜ëŠ” íš¨ê³¼
        - ì„ íƒì ì¸ ë³€ìˆ˜ë§Œ ë‚¨ê¸°ê³  ì‹¶ì„ ë•Œ ìœ ìš©
    
    - L2, ë¦¿ì§€
        - ê°€ì¤‘ì¹˜ì˜ ì œê³±í•©
        - ê°€ì¤‘ì¹˜ë¥¼ ì¤„ì´ì§€ë§Œ, 0ì€ ì•„ë‹˜ -> ì¡°ê¸ˆì”© ì‘ê²Œ
        - ë¶€ë“œëŸ½ê²Œ ì¡°ì •
        - ëª¨ë“  í”¼ì³ë¥¼ ì¡°ê¸ˆì”© ì‚¬ìš©í•˜ê³  ì‹¶ì„ ë•Œ ìœ ìš©
    
    - Elastic Net
        - L1 + L2 ë‘˜ ë‹¤ í•©ì³ ë†“ì€ ê²ƒ
        - L1ì²˜ëŸ¼ ì¼ë¶€ ë³€ìˆ˜ëŠ” ì œê±°í•˜ê³  L2ì²˜ëŸ¼ ë‚˜ë¨¸ì§€ëŠ” ë¶€ë“œëŸ½ê²Œ ì¤„ì„
        - ë°ì´í„° í”¼ì³ê°€ ë§ê±°ë‚˜ í”¼ì³ë¼ë¦¬ ìƒê´€ê´€ê³„ê°€ ìˆì„ ë•Œ ì•„ì£¼ ê°•ë ¥

- Softmaxë€?

    ì—¬ëŸ¬ ê°œì˜ ì•–ì„ ë°›ì•„ì„œ ê°ê°ì„ í™•ë¥ ì²˜ëŸ¼ ë³€í™˜

### ë‹¤ë¥¸ solver
![ì´ë¯¸ì§€](./img/04291544.png)

### max_iter = 500
- max_iterë€ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì •í•˜ëŠ” ì˜µì…˜
- ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ë ¤ê³  ê°€ì¤‘ì¹˜ë¥¼ ê³„ì† ë°”ê¿ˆ, ì´ ê³¼ì •ì´ ë°˜ë³µ
- ì†ì‹¤í•¨ìˆ˜ë€?
    - ëª¨ë¸ì€ ê°€ì¤‘ì¹˜ë¥¼ ëœë¤ìœ¼ë¡œ ê°€ì§€ê³  ìˆìŒ
    - ì´ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ê¸ˆì”© ì¡°ì •í•´ì„œ ì˜ˆì¸¡ì´ ì˜ ë§ê²Œ ë§Œë“œëŠ”ê²Œ ì¤‘ìš”
    - ì†ì‹¤í•¨ìˆ˜ëŠ” ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ì‚¬ì´ì— ì˜¤ì°¨ë¥¼ ìˆ˜ì¹˜ë¡œ í‘œí˜„í•œê²ƒ

"ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ ìµœëŒ€ 500ë²ˆê¹Œì§€ë§Œ ë°˜ë³µí•˜ê³ , ê·¸ ì „ì— ìˆ˜ë ´í•˜ë©´ ì¼ì° ëë‚´ì."

### random_state = 1
- ëœë¤ ìš”ì†Œê°€ ë“¤ì–´ê°€ëŠ” ê³¼ì •ì—ì„œ ë§¤ë²ˆ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ê³  ì‹¶ì„ ë•Œ ê³ ì •í•˜ëŠ” ìˆ«ì
- ìˆ«ìë¥¼ ê³ ì •í•´ì£¼ë©´ í•­ìƒ ë˜‘ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì™€ì„œ ì‹¤í—˜ ì¬í˜„ ê°€ëŠ¥

## KNN

- ì£¼ë³€ì— ìˆëŠ” ë°ì´í„°(Kê°œ)ë¥¼ ì°¾ì•„ì„œ ë‹¤ìˆ˜ê²°ë¡œ ë¶„ë¥˜ or í‰ê· ì„ ë‚´ì„œ íšŒì‰¬í•˜ëŠ” ë°©ë²•

```py
scoreListknn = []
for i in range(1,21):
    KNclassifier = KNeighborsClassifier(n_neighbors = i)
    KNclassifier.fit(X_train, y_train)
    scoreListknn.append(KNclassifier.score(X_test, y_test))
    
plt.plot(range(1,21), scoreListknn)
plt.xticks(np.arange(1,21,1))
plt.xlabel("K value")
plt.ylabel("Score")
plt.show()
KNAcc = max(scoreListknn)
print("KNN best accuracy: {:.2f}%".format(KNAcc*100))
```

### scoreLisftKnn = []
kê°’ì— ë”°ë¥¸ ì •í™•ë„ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ìƒì„±

### for ë¬¸
kê°’ì„ 1ë¶€í„° 20ê¹Œì§€ ë°”ê¿”ê°€ë©´ì„œ ê·¸ë•Œê·¸ë•Œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , í…ŒìŠ¤íŠ¸ì…‹ ì •í™•ë„ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥

### ì‹œê°í™”
kë³„ ì •í™•ë„ ì‹œê°í™”

## SVM

- ë°ì´í„°ë¥¼ ë¶„ë¥˜í•  ë•Œ, ë‘ í´ë˜ìŠ¤ë¥¼ ê°€ì¥ ê¹”ë”í•˜ê²Œ êµ¬ë¶„í•˜ëŠ” ê²½ê³„ì„ ì„ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜
- í´ë˜ìŠ¤ ê°„ì˜ ê°„ê²©ì„ ìµœëŒ€í™” í•˜ëŠ” ì„ ì„ ì°¾ëŠ” ê²ƒ
- ì¥ì  : ê°•ë ¥í•œ ì´ë¡ , ê³ ì°¨ì›ì— ê°•í•¨
- ë‹¨ì  : ëŠë¦¼, íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”
- ì„ í˜• SVM : ë°ì´í„°ê°€ ì„ í˜•ìœ¼ë¡œ êµ¬ë¶„ ê°€ëŠ¥í• ë•Œ
- ì»¤ë„ SVM : ì„ í˜•ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ì—†ì„ ë•Œ

```py
SVCclassifier = SVC(kernel='rbf', max_iter=500)
SVCclassifier.fit(X_train, y_train)

y_pred = SVCclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
SVCAcc = accuracy_score(y_pred,y_test)
print('SVC accuracy: {:.2f}%'.format(SVCAcc*100))
```

### SVCclassifier = SVC(kernel = 'rbf', max_iter = 500)
- SVC = svc ê°ì²´ ìƒì„±
    - kernel : rbfë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ëœ» -> ë¹„ì„ í˜• ê²½ê³„ í•™ìŠµ ê°€ëŠ¥ (ë™ê·¸ë—ê±°ë‚˜ ê¼¬ë¶ˆí•œ ê±°)
    - max_iter : ìµœì í™”ë¥¼ ìµœëŒ€ 500ë²ˆ ë°˜ë³µ

## Naive Bayes (Gaussian)
- í™•ë¥ ì— ê¸°ë°˜í•´ì„œ ë¶„ë¥˜í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜
- íŠ¹ì„±ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •
- Naive : ê° í”¼ì³ë¼ë¦¬ ì™„ì „íˆ ë…ë¦½
- Gaussian : ê° íŠ¹ì„±ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„

![ì´ë¯¸ì§€](./img/04291609.png)

```py
NBclassifier2 = GaussianNB()
NBclassifier2.fit(X_train, y_train)

y_pred = NBclassifier2.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
NBAcc2 = accuracy_score(y_pred,y_test)
print('Gaussian Naive Bayes accuracy: {:.2f}%'.format(NBAcc2*100))
```

### fit
ì—¬ê¸°ì„œ í•™ìŠµì´ë€ ê° í´ë˜ìŠ¤ë³„ë¡œ ê° í”¼ì³ì˜ í˜•ê· ê³¼ ë¶„ì‚°ì„ ê³„ì‚°

### y_pred
í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í´ë˜ìŠ¤ ì„ íƒ

## Decision Tree

```py
scoreListDT = []
for i in range(2,21):
    DTclassifier = DecisionTreeClassifier(max_leaf_nodes=i)
    DTclassifier.fit(X_train, y_train)
    scoreListDT.append(DTclassifier.score(X_test, y_test))
    
plt.plot(range(2,21), scoreListDT)
plt.xticks(np.arange(2,21,1))
plt.xlabel("Leaf")
plt.ylabel("Score")
plt.show()
DTAcc = max(scoreListDT)
print("Decision Tree Accuracy: {:.2f}%".format(DTAcc*100))
```

### for ë¬¸
ë¦¬í”„ì˜ ê°œìˆ˜ë¥¼ 2ë¶€í„° 20ê¹Œì§€ ë°˜ë³µí•˜ë©´ì„œ, ê° ê°œìˆ˜ë³„ testì…‹ ì •í™•ë„ ê¸°ë¡

ë¦¬í”„ë€? : ê²°ì •ì´ ìµœì¢…ì ìœ¼ë¡œ ë‚´ë ¤ì§€ëŠ” ì¢…ì°©ì 

## Random Forest

```py
scoreListRF = []
for i in range(2,25):
    RFclassifier = RandomForestClassifier(n_estimators = 1000, random_state = 1, max_leaf_nodes=i)
    RFclassifier.fit(X_train, y_train)
    scoreListRF.append(RFclassifier.score(X_test, y_test))
    
plt.plot(range(2,25), scoreListRF)
plt.xticks(np.arange(2,25,1))
plt.xlabel("RF Value")
plt.ylabel("Score")
plt.show()
RFAcc = max(scoreListRF)
print("Random Forest Accuracy:  {:.2f}%".format(RFAcc*100))
```

### forë¬¸
- ìµœëŒ€ ë¦¬í”„ë¥¼ 2ë¶€í„° 24ê°œë¡œ ì œí•œí•œ ëª¨ë¸ì„ ë§Œë“¤ê³  í…ŒìŠ¤íŠ¸ì…‹ ê²°ê³¼ë¥¼ ê¸°ë¡

### n_estimators = 1000
- ìƒì„±í•  ê²°ì • íŠ¸ë¦¬ì˜ ê°œìˆ˜ (1000ê°œ)

## Gradient Boosting
ì—¬ëŸ¬ê°œì˜ ì•½í•œ ëª¨ë¸(ì–•ì€ ê²°ì •íŠ¸ë¦¬)ì„ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ë©´ì„œ, ì´ì „ ëª¨ë¸ì´ ëª» ë§ì¶˜ ë¶€ë¶„(ì˜¤ì°¨)ì„ ë³´ì™„í•´ ë‚˜ê°

"ì˜ ëª»í•œ ê±¸ ê³„ì† ìˆ˜ì •í•˜ë©´ì„œ ê³¼ì œë¥¼ ì™„ì„±í•˜ëŠ”ê²ƒ"

1. ì²˜ìŒ ì‹œì‘ : ëŒ€ì¶© ì „ì²´ë¥¼ ë³´ê³  ì˜ˆì¸¡
2. ì˜¤ì°¨ ê³„ì‚°
3. ì˜¤ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ë‹¤ìŒ ëª¨ë¸ í•™ìŠµ
4. ì˜ˆì¸¡ ê²°ê³¼ ê°±ì‹ 
5. ë°˜ë³µ

### íŒŒë¼ë¯¸í„°

n_estimators : ëª‡ ê°œì˜ íŠ¸ë¦¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë§Œë“¤ê¹Œ?

learning_rate : ê° íŠ¸ë¦¬ì˜ ê¸°ì—¬ë„ë¥¼ ì–¼ë§ˆë‚˜ ì¤„ê¹Œ? (ì‘ìœ¼ë©´ ì²œì²œíˆ, ì‘ì•„í– ê³¼ì í•© ë°©ì§€)

max_depth : ê° íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´

sumsample : ê° íŠ¸ë¦¬ í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„° ë¹„ìœ¨ (1ì´ë©´ ì „ì²´, 0.5ë©´ ë¬´ì‘ìœ„ ì ˆë°˜ -> ì•™ìƒë¸” íš¨ê³¼)

max_leaf_nodes : í•˜ë‚˜ì˜ íŠ¸ë¦¬ ì•ˆì—ì„œ ë¦¬í”„ ë…¸íŠ¸ ìµœëŒ€ ìˆ˜

```py
paramsGB={'n_estimators':[100,200,300,400,500],
      'max_depth':[1,2,3,4,5],
      'subsample':[0.5,1],
      'max_leaf_nodes':[2,5,10,20,30,40,50]}
```

```py
GB = RandomizedSearchCV(GradientBoostingClassifier(), paramsGB, cv=20)
GB.fit(X_train, y_train)
```

### RandomizedSearchCV
- GridSearchCVì™€ ë‹¤ë¥´ê²Œ ëœë¤í•˜ê²Œ ì¼ë¶€ ì¡°í•©ë§Œ ì‹œë„
- í›„ë³´ ì¡°í•©ì´ ë§ì„ ë•Œ ì‚¬ìš©

### CV = 20
- êµì°¨ê²€ì¦(20)ì„ í†µí•´ ê°€ì¥ ì¢‹ì€ ì¡°í•©ì„ ì°¾ëŠ”ê²ƒ
- ë°ì´í„°ë¥¼ 20ë“±ë¶„í•´ì„œ, 20ë²ˆ í•™ìŠµ + í‰ê°€

```py
GBclassifier = GradientBoostingClassifier(subsample=0.5, n_estimators=400, max_depth=4, max_leaf_nodes=10)
GBclassifier.fit(X_train, y_train)

y_pred = GBclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
GBAcc = accuracy_score(y_pred,y_test)
print('Gradient Boosting accuracy: {:.2f}%'.format(GBAcc*100))
```
ì•ì—ì„œ ì°¾ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ í‰ê°€

### ê° ë¶€ìŠ¤íŠ¸ ê³„ì—´ ëª¨ë¸ë§ ë¹„êµ
![ì´ë¯¸ì§€](./img/04291632.png)
![ì´ë¯¸ì§€](./img/04291634.png)

"XGBoostëŠ” ê°•ë ¥í•˜ê³ , LightGBMì€ ë¹ ë¥´ê³ , CatBoostëŠ” ë˜‘ë˜‘í•˜ë‹¤."